{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d60fb8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "12.6\n",
      "2.6.0+cu126\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import json\n",
    "from datasets import Dataset\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# For chinese word segmentation\n",
    "import jieba\n",
    "import re\n",
    "\n",
    "import torch\n",
    "# ML - Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# ML - Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# LLMs\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "# 12.9.90 and below available\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d08754e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weibo_long_text_posts_chinese.csv cleaning completed\n",
      "weibo_COVID_news_posts_chinese.csv cleaning completed\n",
      "weibo_comment_posts_chinese.csv cleaning completed\n"
     ]
    }
   ],
   "source": [
    "# Clean csv datasets, keep only text and labels\n",
    "csv_file_list = [\n",
    "    'weibo_long_text_posts_chinese.csv',\n",
    "    'weibo_COVID_news_posts_chinese.csv',\n",
    "    'weibo_comment_posts_chinese.csv'\n",
    "]\n",
    "\n",
    "def clean_csv(input_file):\n",
    "    df = pd.read_csv(input_file)\n",
    "    # rename the cleaned csv file by add '_cleaned'\n",
    "    file_name = input_file.split('.')\n",
    "    output_file = f'{\".\".join(file_name[:-1])}_cleaned.{file_name[-1]}'\n",
    "    # keep only text and labels\n",
    "    text_column = 'text'\n",
    "    label_column = 'label'\n",
    "    df_cleaned = df[[text_column, label_column]].copy()\n",
    "    # simple data cleaning\n",
    "    df_cleaned = df_cleaned.dropna(subset=[text_column])\n",
    "    df_cleaned = df_cleaned[df_cleaned[text_column].str.strip() != '']\n",
    "    df_cleaned = df_cleaned[df_cleaned[label_column].astype(int).isin([0, 1])]\n",
    "    # write to new csv file\n",
    "    df_cleaned.to_csv(output_file, index=False, encoding='utf-8')\n",
    "    print(f'{input_file} cleaning completed')\n",
    "\n",
    "# for loop to clean multiple datasets\n",
    "for file in csv_file_list:\n",
    "    clean_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c023796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  【#崔天凯称外媒抹黑中国援助物资是ABC思维#：Anything But China】新冠病...      1\n",
      "1  【#香港失业率5.2%创十年新高#】香港特区政府统计处19日公布，2月至4月经季节性调整的失...      1\n",
      "2  【泪目！#9分钟的中国抗疫图卷#，你看见自己了吗？】这张#中国抗疫图卷#，时长9分钟，它记录...      1\n",
      "3  【#美国新冠肺炎超221万#：#美国日新增确诊超3万例#】据美国约翰斯·霍普金斯大学疫情实时...      1\n",
      "4  【#钟南山称不从全球范围内控制好不可能战胜疫情#】3月12日，广东省人民政府新闻办公室举行新...      1\n",
      "5  【正在直播：#杭州通报最新疫情防控工作#】根据杭州市新型冠状病毒肺炎疫情防控指挥部工作要求，...      1\n",
      "6  【#日本全国紧急状态将延长#】据日本广播协会（NHK）电视台统计，截至4日10时30分（北京...      1\n",
      "7  【千里为邻，战疫必胜！#湖北捐助黑龙江首批医用物资#启程赴绥芬河】15日11：06，湖北向黑...      1\n",
      "8  【继续加油！#北京连续4天零新增#：#北京中风险地区15个#】7月9日0时至24时，北京无新...      1\n",
      "9  【#习近平同美国总统特朗普通电话#】国家主席习近平27日应约同美国总统特朗普通电话。　　习近...      1\n"
     ]
    }
   ],
   "source": [
    "csv_file = \"weibo_COVID_news_posts_chinese_cleaned.csv\"\n",
    "# 1:real news 0:conspiracy theory\n",
    "df = pd.read_csv(csv_file, nrows=10)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "743aec73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  长时间大强度的运动，会导致身体机能失调，免疫功能下降，并且运动损伤风险增加。因此，特别忌讳平...      0\n",
      "1  因现有研究显示ACE2是新型冠状病毒入侵人体的关键，网传服用ACEI（血管紧张素转化酶抑制剂...      0\n",
      "2  在居家防疫期间，为确保运动安全有效，运动强度必须适宜。强度过低，没有锻炼效果，但是长时间大强...      0\n",
      "3  有传闻称：「病患遗体解剖发现死者肺部出现大量痰栓，而痰栓是由呼吸机使用所产生，致人缺氧而死。...      0\n",
      "4  近日，有人在朋友圈兜售某公司生产的新冠病毒抗体检测试剂盒，单价150元，并宣称可以家庭自行使...      0\n",
      "5  因现有研究显示ACE2是新型冠状病毒入侵人体的关键，网传服用ACEI（血管紧张素转化酶抑制剂...      0\n",
      "6  网传的新闻截图原文「CDCconfirmsfirstcoronaviruscaseof\"un...      0\n",
      "7  28日，科技部社会发展科技司司长吴远彬表示，目前研究结论显示，呼吸道飞沫和密切接触传播仍然是...      0\n",
      "8  据香港文汇网报道，香港渔护署发现一名新冠肺炎确诊患者饲养的宠物狗对病毒测试呈弱阳性反应。香港...      1\n",
      "9  近日，部分网民转发「乐陵十三人染sk5病毒，参与抢救的医生已被隔离」的信息，其实，该谣言早在...      0\n"
     ]
    }
   ],
   "source": [
    "csv_file = \"weibo_long_text_posts_chinese_cleaned.csv\"\n",
    "# 1:real posts 0:conspiracy theory\n",
    "df = pd.read_csv(csv_file, nrows=10)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3175bba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  人间惨剧：今天下午约14点，宁波妇儿医院，一妇女携带一婴儿在住院楼跳楼，后抢救无效死亡。具体...      0\n",
      "1  再去武大，已无牌坊！非要拆掉？@章立凡 @袁裕来律师 @老徐时评 @徐昕 @杨锦麟 @左小祖...      0\n",
      "2  中国最美丽的乡村\"江西婺源\"一\"教师打死学生\" 昨晚，在被誉为中国最美丽的乡村江西省婺源县清...      0\n",
      "3  忍者QS：江苏省东海县女镇党委书记徐艳，因不愿陪县委书记关永健上床，竟然被警察毒打致子宫破裂...      0\n",
      "4  《北大猛男，持刀刺官！！！》“可歌可泣”的是王同学投案自首之后冷冷说了一句话是 “我并不后悔...      0\n",
      "5  好心人帮忙转发下！　　　　　　　 　 　昨日福建省泉州市警察局抓到几个拐卖小孩犯罪团伙，现场...      0\n",
      "6  【怎么鉴定地沟油】炒菜时放一颗剥皮的蒜头(蒜子)，蒜子对黄曲霉素最敏感。如果蒜子变红色就是地...      0\n",
      "7  湖南省交通厅原副厅长李晓希在今年两会上说；目前，我国《刑法》对贪污受贿量刑太轻了。如果贪污受...      0\n",
      "8                                           让历史照进现实！      1\n",
      "9  转来的，有懂阿拉伯语的吗，给翻译翻译！——叙利亚标语：中国，你们的道德比你们的产品还垃圾 ！...      0\n"
     ]
    }
   ],
   "source": [
    "csv_file = \"weibo_comment_posts_chinese_cleaned.csv\"\n",
    "# 1:real posts 0:conspiracy theory\n",
    "df = pd.read_csv(csv_file, nrows=10)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bd8c578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process and word segmentation for chinese text\n",
    "def process_chinese_text(text):\n",
    "    # remove all non-chinese characters (\\u4e00-\\u9fa5) and non-english characters (a-zA-Z)\n",
    "    text = re.sub(r'[^\\u4e00-\\u9fa5a-zA-Z]', ' ', text)\n",
    "    # split continuous chinese text into separate words there's no space between chinese words\n",
    "    words = jieba.lcut(text)\n",
    "    # remove words that have no real meaning\n",
    "    stopwords = {'的', '地', '得', '了', '着', '呢', '吗', '吧', '呀', '啊', '把', '被', '对', '往', '从', '由', '为', '给'}\n",
    "    words = [w for w in words if len(w) > 1 or (len(w) == 1 and w not in stopwords)]\n",
    "    return \" \".join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f910a69",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ML - Logistic Regression\n",
    "def logistic_regression_score(input_file):\n",
    "    df = pd.read_csv(input_file)\n",
    "    df['process_text'] = df['text'].apply(process_chinese_text)\n",
    "    X = df['process_text']\n",
    "    y = df['label']\n",
    "\n",
    "    # train set and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)\n",
    "\n",
    "    # TfidfVectorizer for convert text to numerical features\n",
    "    # Logistic regression cannot directly process text\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train_vectorizer = vectorizer.fit_transform(X_train)\n",
    "    X_test_vectorizer = vectorizer.transform(X_test)\n",
    "\n",
    "    # train logistic regression model\n",
    "    logistic_regression = LogisticRegression()\n",
    "    logistic_regression.fit(X_train_vectorizer, y_train)\n",
    "\n",
    "    # f1 score and accuracy\n",
    "    y_predict = logistic_regression.predict(X_test_vectorizer)\n",
    "    f1 = f1_score(y_test, y_predict, average='weighted')\n",
    "    accuracy = accuracy_score(y_test, y_predict)\n",
    "    precision = precision_score(y_test, y_predict, average='weighted')\n",
    "    recall = recall_score(y_test, y_predict, average='weighted')\n",
    "\n",
    "    print(f'f1 score of Logistic Regression about {input_file}: {f1}')\n",
    "    print(f'accuracy of Logistic Regression about {input_file}: {accuracy}')\n",
    "    print(f'precision of Logistic Regression about {input_file}: {precision}')\n",
    "    print(f'recall of Logistic Regression about {input_file}: {recall}')\n",
    "    return f1, accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8893b74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML - Random Forest\n",
    "def random_forest(input_file):\n",
    "    df = pd.read_csv(input_file)\n",
    "    df['process_text'] = df['text'].apply(process_chinese_text)\n",
    "    X = df['process_text']\n",
    "    y = df['label']\n",
    "\n",
    "    # train set and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)\n",
    "\n",
    "    # TfidfVectorizer for convert text to numerical features\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train_vect = vectorizer.fit_transform(X_train)\n",
    "    X_test_vect = vectorizer.transform(X_test)\n",
    "\n",
    "    # train random forest model\n",
    "    ranfor = RandomForestClassifier(random_state=50)\n",
    "    ranfor.fit(X_train_vect, y_train)\n",
    "\n",
    "    # f1 score and accuracy\n",
    "    y_predict = ranfor.predict(X_test_vect)\n",
    "    f1 = f1_score(y_test, y_predict, average='weighted')\n",
    "    accuracy = accuracy_score(y_test, y_predict)\n",
    "    precision = precision_score(y_test, y_predict, average='weighted')\n",
    "    recall = recall_score(y_test, y_predict, average='weighted')\n",
    "\n",
    "    print(f'f1 score of Random Forest about {input_file}: {f1}')\n",
    "    print(f'accuracy of Random Forest about {input_file}: {accuracy}')\n",
    "    print(f'precision of Random Forest about {input_file}: {precision}')\n",
    "    print(f'recall of Random Forest about {input_file}: {recall}')\n",
    "    return f1, accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6f1b6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLMs - BERT\n",
    "# Some of the ideas come from https://zhuanlan.zhihu.com/p/700074905\n",
    "def LLMs_BERT(input_file):\n",
    "    df = pd.read_csv(input_file)\n",
    "    X = df['text']\n",
    "    y = df['label']\n",
    "\n",
    "    # train set and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)\n",
    "\n",
    "    # load BERT pre-trained word tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "    # convert text into an acceptable input for the model\n",
    "    def tokenize(batch):\n",
    "        return tokenizer(batch['text'], padding='max_length', truncation=True, max_length=256)\n",
    "\n",
    "    # convert Pandas DataFrame to Dataset format\n",
    "    train_df = pd.DataFrame({'text': X_train, 'labels': y_train})\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    test_df = pd.DataFrame({'text': X_test, 'labels': y_test})\n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "    train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "    test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "    \n",
    "    # load model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained('bert-base-chinese', num_labels=2)\n",
    "    model = model.to('cuda')\n",
    "\n",
    "    # set train parameters\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results/bert',\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        report_to='none',\n",
    "        no_cuda=False,\n",
    "        fp16=True,\n",
    "        dataloader_pin_memory=True\n",
    "    )\n",
    "\n",
    "    # a functions for calculating model evaluation metrics\n",
    "    def compute_metrics(eval_prediction):\n",
    "        predictions, labels = eval_prediction\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        return {'f1' : f1_score(labels, predictions, average='weighted'), \n",
    "                'accuracy' : accuracy_score(labels, predictions), \n",
    "                'precision': precision_score(labels, predictions, average='weighted'), \n",
    "                'recall': recall_score(labels, predictions, average='weighted')}\n",
    "\n",
    "    # set train parameters\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate()\n",
    "    \n",
    "    f1 = eval_results['eval_f1']\n",
    "    accuracy = eval_results['eval_accuracy']\n",
    "    precision = eval_results['eval_precision']\n",
    "    recall = eval_results['eval_recall']\n",
    "    print(f'f1 score of BERT about {input_file}: {f1}')\n",
    "    print(f'accuracy of BERT about {input_file}: {accuracy}')\n",
    "    print(f'precision of BERT about {input_file}: {precision}')\n",
    "    print(f'recall of BERT about {input_file}: {recall}')\n",
    "    return f1, accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b1c997b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLMs - RoBERTa\n",
    "def LLMs_RoBERTa(input_file):\n",
    "    df = pd.read_csv(input_file)\n",
    "    X = df['text']\n",
    "    y = df['label']\n",
    "\n",
    "    # train set and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)\n",
    "\n",
    "    # load BERT pre-trained word tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "\n",
    "    # convert text into an acceptable input for the model\n",
    "    def tokenize(batch):\n",
    "        return tokenizer(batch['text'], padding='max_length', truncation=True, max_length=256)\n",
    "\n",
    "    # convert Pandas DataFrame to Dataset format\n",
    "    train_df = pd.DataFrame({'text': X_train, 'labels': y_train})\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    test_df = pd.DataFrame({'text': X_test, 'labels': y_test})\n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "    train_dataset = train_dataset.map(tokenize, batched=True).with_format(\n",
    "        type='torch', \n",
    "        columns=['input_ids', 'attention_mask', 'labels']\n",
    "    )\n",
    "    test_dataset = test_dataset.map(tokenize, batched=True).with_format(\n",
    "        type='torch', \n",
    "        columns=['input_ids', 'attention_mask', 'labels']\n",
    "    )\n",
    "\n",
    "    # load model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained('hfl/chinese-roberta-wwm-ext', num_labels=2)\n",
    "    model = model.to('cuda')\n",
    "\n",
    "    # set train parameters\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results/roberta',\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        report_to='none',\n",
    "        no_cuda=False,\n",
    "        fp16=True,\n",
    "        dataloader_pin_memory=True\n",
    "    )\n",
    "\n",
    "    # a functions for calculating model evaluation metrics\n",
    "    def compute_metrics(eval_prediction):\n",
    "        predictions, labels = eval_prediction\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        return {'f1' : f1_score(labels, predictions, average='weighted'), \n",
    "                'accuracy' : accuracy_score(labels, predictions), \n",
    "                'precision': precision_score(labels, predictions, average='weighted'), \n",
    "                'recall': recall_score(labels, predictions, average='weighted')}\n",
    "\n",
    "    # set train parameters\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate()\n",
    "    \n",
    "    f1 = eval_results['eval_f1']\n",
    "    accuracy = eval_results['eval_accuracy']\n",
    "    precision = eval_results['eval_precision']\n",
    "    recall = eval_results['eval_recall']\n",
    "    print(f'f1 score of RoBERTa about {input_file}: {f1}')\n",
    "    print(f'accuracy of RoBERTa about {input_file}: {accuracy}')\n",
    "    print(f'precision of RoBERTa about {input_file}: {precision}')\n",
    "    print(f'recall of RoBERTa about {input_file}: {recall}')\n",
    "    return f1, accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b07b5a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score of Logistic Regression about weibo_COVID_news_posts_chinese_cleaned.csv: 0.8710155034439545\n",
      "accuracy of Logistic Regression about weibo_COVID_news_posts_chinese_cleaned.csv: 0.8931116389548693\n",
      "precision of Logistic Regression about weibo_COVID_news_posts_chinese_cleaned.csv: 0.9052580436190888\n",
      "recall of Logistic Regression about weibo_COVID_news_posts_chinese_cleaned.csv: 0.8931116389548693\n",
      "f1 score of Random Forest about weibo_COVID_news_posts_chinese_cleaned.csv: 0.9174519438432565\n",
      "accuracy of Random Forest about weibo_COVID_news_posts_chinese_cleaned.csv: 0.9263657957244655\n",
      "precision of Random Forest about weibo_COVID_news_posts_chinese_cleaned.csv: 0.9323413463332131\n",
      "recall of Random Forest about weibo_COVID_news_posts_chinese_cleaned.csv: 0.9263657957244655\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1683 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/421 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='633' max='633' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [633/633 01:15, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.050500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53' max='53' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [53/53 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score of BERT about weibo_COVID_news_posts_chinese_cleaned.csv: 0.9976178482547534\n",
      "accuracy of BERT about weibo_COVID_news_posts_chinese_cleaned.csv: 0.997624703087886\n",
      "precision of BERT about weibo_COVID_news_posts_chinese_cleaned.csv: 0.9976314510904772\n",
      "recall of BERT about weibo_COVID_news_posts_chinese_cleaned.csv: 0.997624703087886\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1683 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/421 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-roberta-wwm-ext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='633' max='633' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [633/633 01:27, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.045500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53' max='53' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [53/53 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score of RoBERTa about weibo_COVID_news_posts_chinese_cleaned.csv: 0.9976178482547534\n",
      "accuracy of RoBERTa about weibo_COVID_news_posts_chinese_cleaned.csv: 0.997624703087886\n",
      "precision of RoBERTa about weibo_COVID_news_posts_chinese_cleaned.csv: 0.9976314510904772\n",
      "recall of RoBERTa about weibo_COVID_news_posts_chinese_cleaned.csv: 0.997624703087886\n",
      "f1 score of Logistic Regression about weibo_long_text_posts_chinese_cleaned.csv: 0.8877627627627626\n",
      "accuracy of Logistic Regression about weibo_long_text_posts_chinese_cleaned.csv: 0.8977777777777778\n",
      "precision of Logistic Regression about weibo_long_text_posts_chinese_cleaned.csv: 0.9031339031339032\n",
      "recall of Logistic Regression about weibo_long_text_posts_chinese_cleaned.csv: 0.8977777777777778\n",
      "f1 score of Random Forest about weibo_long_text_posts_chinese_cleaned.csv: 0.9180076628352491\n",
      "accuracy of Random Forest about weibo_long_text_posts_chinese_cleaned.csv: 0.9222222222222223\n",
      "precision of Random Forest about weibo_long_text_posts_chinese_cleaned.csv: 0.9229629629629629\n",
      "recall of Random Forest about weibo_long_text_posts_chinese_cleaned.csv: 0.9222222222222223\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1797 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/450 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='675' max='675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [675/675 01:43, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.112800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/57 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score of BERT about weibo_long_text_posts_chinese_cleaned.csv: 0.9800354450921359\n",
      "accuracy of BERT about weibo_long_text_posts_chinese_cleaned.csv: 0.98\n",
      "precision of BERT about weibo_long_text_posts_chinese_cleaned.csv: 0.9800845414054301\n",
      "recall of BERT about weibo_long_text_posts_chinese_cleaned.csv: 0.98\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1797 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/450 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-roberta-wwm-ext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='675' max='675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [675/675 01:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.091900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/57 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score of RoBERTa about weibo_long_text_posts_chinese_cleaned.csv: 0.9822222222222222\n",
      "accuracy of RoBERTa about weibo_long_text_posts_chinese_cleaned.csv: 0.9822222222222222\n",
      "precision of RoBERTa about weibo_long_text_posts_chinese_cleaned.csv: 0.9822222222222222\n",
      "recall of RoBERTa about weibo_long_text_posts_chinese_cleaned.csv: 0.9822222222222222\n",
      "f1 score of Logistic Regression about weibo_comment_posts_chinese_cleaned.csv: 0.5930445913262307\n",
      "accuracy of Logistic Regression about weibo_comment_posts_chinese_cleaned.csv: 0.6238938053097345\n",
      "precision of Logistic Regression about weibo_comment_posts_chinese_cleaned.csv: 0.6139880291099817\n",
      "recall of Logistic Regression about weibo_comment_posts_chinese_cleaned.csv: 0.6238938053097345\n",
      "f1 score of Random Forest about weibo_comment_posts_chinese_cleaned.csv: 0.6005799199259731\n",
      "accuracy of Random Forest about weibo_comment_posts_chinese_cleaned.csv: 0.6209439528023599\n",
      "precision of Random Forest about weibo_comment_posts_chinese_cleaned.csv: 0.6094679760139914\n",
      "recall of Random Forest about weibo_comment_posts_chinese_cleaned.csv: 0.6209439528023599\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2709 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/678 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1017' max='1017' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1017/1017 02:26, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.637800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.488900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='85' max='85' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [85/85 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score of BERT about weibo_comment_posts_chinese_cleaned.csv: 0.6676187879448794\n",
      "accuracy of BERT about weibo_comment_posts_chinese_cleaned.csv: 0.6710914454277286\n",
      "precision of BERT about weibo_comment_posts_chinese_cleaned.csv: 0.6669990397466674\n",
      "recall of BERT about weibo_comment_posts_chinese_cleaned.csv: 0.6710914454277286\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2709 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/678 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-roberta-wwm-ext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1017' max='1017' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1017/1017 02:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.624600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.457500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='85' max='85' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [85/85 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score of RoBERTa about weibo_comment_posts_chinese_cleaned.csv: 0.6718806408151193\n",
      "accuracy of RoBERTa about weibo_comment_posts_chinese_cleaned.csv: 0.6740412979351033\n",
      "precision of RoBERTa about weibo_comment_posts_chinese_cleaned.csv: 0.6710270369213914\n",
      "recall of RoBERTa about weibo_comment_posts_chinese_cleaned.csv: 0.6740412979351033\n",
      "                              dataset_name               model  f1 score  accuracy  precison   recall\n",
      "weibo_COVID_news_posts_chinese_cleaned.csv Logistic Regression  0.871016  0.893112  0.905258 0.893112\n",
      "weibo_COVID_news_posts_chinese_cleaned.csv       Random Forest  0.917452  0.926366  0.932341 0.926366\n",
      "weibo_COVID_news_posts_chinese_cleaned.csv        BERT-chinese  0.997618  0.997625  0.997631 0.997625\n",
      "weibo_COVID_news_posts_chinese_cleaned.csv     RoBERTa-chinese  0.997618  0.997625  0.997631 0.997625\n",
      " weibo_long_text_posts_chinese_cleaned.csv Logistic Regression  0.887763  0.897778  0.903134 0.897778\n",
      " weibo_long_text_posts_chinese_cleaned.csv       Random Forest  0.918008  0.922222  0.922963 0.922222\n",
      " weibo_long_text_posts_chinese_cleaned.csv        BERT-chinese  0.980035  0.980000  0.980085 0.980000\n",
      " weibo_long_text_posts_chinese_cleaned.csv     RoBERTa-chinese  0.982222  0.982222  0.982222 0.982222\n",
      "   weibo_comment_posts_chinese_cleaned.csv Logistic Regression  0.593045  0.623894  0.613988 0.623894\n",
      "   weibo_comment_posts_chinese_cleaned.csv       Random Forest  0.600580  0.620944  0.609468 0.620944\n",
      "   weibo_comment_posts_chinese_cleaned.csv        BERT-chinese  0.667619  0.671091  0.666999 0.671091\n",
      "   weibo_comment_posts_chinese_cleaned.csv     RoBERTa-chinese  0.671881  0.674041  0.671027 0.674041\n"
     ]
    }
   ],
   "source": [
    "# Use a loop to iterate over multiple data sets\n",
    "datasets_list = [\n",
    "    'weibo_COVID_news_posts_chinese_cleaned.csv',\n",
    "    'weibo_long_text_posts_chinese_cleaned.csv',\n",
    "    'weibo_comment_posts_chinese_cleaned.csv'\n",
    "]\n",
    "# new list to store the scores output by the function\n",
    "score_results = []\n",
    "\n",
    "for dataset in datasets_list:\n",
    "    logistic_regression_f1, logistic_regression_accuracy, logistic_regression_precison, logistic_regression_recall = logistic_regression_score(dataset)\n",
    "    random_forest_f1, random_forest_accuracy, random_forest_precsion, random_forest_recall = random_forest(dataset)\n",
    "    BERT_f1, BERT_accuracy, BERT_precsion, BERT_recall = LLMs_BERT(dataset)\n",
    "    RoBERTa_f1, RoBERTa_accuracy, RoBERTa_precsion, RoBERTa_recall = LLMs_RoBERTa(dataset)\n",
    "    \n",
    "    # add the results to list\n",
    "    score_results.append({\n",
    "        'dataset_name': dataset,\n",
    "        'model': 'Logistic Regression',\n",
    "        'f1 score': logistic_regression_f1,\n",
    "        'accuracy': logistic_regression_accuracy,\n",
    "        'precison': logistic_regression_precison,\n",
    "        'recall': logistic_regression_recall\n",
    "    })\n",
    "    score_results.append({\n",
    "        'dataset_name': dataset,\n",
    "        'model': 'Random Forest',\n",
    "        'f1 score': random_forest_f1,\n",
    "        'accuracy': random_forest_accuracy,\n",
    "        'precison': random_forest_precsion,\n",
    "        'recall': random_forest_recall\n",
    "    })\n",
    "    score_results.append({\n",
    "        'dataset_name': dataset,\n",
    "        'model': 'BERT-chinese',\n",
    "        'f1 score': BERT_f1,\n",
    "        'accuracy': BERT_accuracy,\n",
    "        'precison': BERT_precsion,\n",
    "        'recall': BERT_recall\n",
    "    })\n",
    "    score_results.append({\n",
    "        'dataset_name': dataset,\n",
    "        'model': 'RoBERTa-chinese',\n",
    "        'f1 score': RoBERTa_f1,\n",
    "        'accuracy': RoBERTa_accuracy,\n",
    "        'precison': RoBERTa_precsion,\n",
    "        'recall': RoBERTa_recall\n",
    "    })\n",
    "    \n",
    "\n",
    "# convert to DataFrame and print the table\n",
    "score_results_df = pd.DataFrame(score_results)\n",
    "print(score_results_df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
